{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e851dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qwc45\\AppData\\Local\\Temp\\ipykernel_18352\\3824536991.py:29: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import csv\n",
    "\n",
    "# import the relevant functions from cnn functions\n",
    "from cnn_functions import prep_training_data, prep_test_data, show_logger_plot\n",
    "from cnn_functions import train_model, test_model, train_aug_gen_model\n",
    "\n",
    "# set the seed value\n",
    "seed = 42\n",
    "\n",
    "# set pythonhashseed to a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "\n",
    "# set the python pseudorandom number generator to a fixed value\n",
    "random.seed(seed)\n",
    "\n",
    "# set the numpy and tf pseudorandom number generator to a fixed value\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# configure new session\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be901bd",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "#### The functions used in this files will be loaded from \"cnn_functions.py\"\n",
    "\n",
    "The code cell below contains the codes to do the following:\n",
    "- Read the csv files containing the file names and the labels\n",
    "- Load the images based on the file names read from the csv file\n",
    "- Prepare the training, validation and test data to be used for the cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4ab398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qwc45\\anaconda3\\envs\\sa4110\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training and testing\n",
    "csv_folder = \"image-info-csv/\"\n",
    "train_csv_fname = f\"{csv_folder}train_refined.csv\"\n",
    "train_folder_name = \"train\"\n",
    "test_csv_fname = f\"{csv_folder}test_base.csv\"\n",
    "test_folder_name = \"test\"\n",
    "\n",
    "# initialise rescale value\n",
    "img_rescale = 1./255\n",
    "validation_split = 0.15\n",
    "reshaped_size=[224,224]\n",
    "\n",
    "# Generate the data for training and testing\n",
    "x_train, x_val, y_train, y_val, x_input_shape, output_layer = prep_training_data(train_csv_fname, train_folder_name\n",
    "                                                                 , validation_split, img_rescale, reshaped_size)\n",
    "\n",
    "x_test, y_test = prep_test_data(test_csv_fname, test_folder_name, img_rescale, reshaped_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2b696",
   "metadata": {},
   "source": [
    "# Create the CNN base model\n",
    "\n",
    "#### The functions used in this files will be loaded from \"cnn_functions.py\"\n",
    "\n",
    "- The code for the base model is shown below. \n",
    "- initialise callbacks to be used in training the model\n",
    "- parameters for the image augmentation will be tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef67a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_untuned_model(x_input_shape, output_layer):\n",
    "    \"\"\"\n",
    "    creates a cnn model\n",
    "\n",
    "    Parameters:\n",
    "    x_input_shape (tuple): the shape of the input data\n",
    "    output_layer (int): the number of neurons in the output layer\n",
    "\n",
    "    Returns:\n",
    "    keras model object.\n",
    "    \"\"\"\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, \n",
    "         kernel_size=(3, 3), activation=\"relu\", input_shape=(x_input_shape), kernel_initializer=initializer))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=initializer))    \n",
    "    model.add(tf.keras.layers.Dense(units=output_layer, activation=\"softmax\", kernel_initializer=initializer))    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52bad7",
   "metadata": {},
   "source": [
    "# Test data image augmentation techniques\n",
    "\n",
    "With reference to the images provided in the train and test dataset, the following image augmentation techniques are chosen to augment the data and to investigate if it will result in an improvement in training the model. The explanation for the chosen methods are provided below.<br>\n",
    "\n",
    "rotation_range: the fruits provided in the dataset provided are not always at the same orientation and should be tested with different rotation angles.<br>\n",
    "\n",
    "width_shift_range: some of the fruits are not located directly in the centre and shifting the positions horizontally may provide more useful training data<br>\n",
    "\n",
    "height_shift_range: adjusting the position of the fruits vertically can help to increase more diversity in the training images and may provide better results<br>\n",
    "\n",
    "shear_range: applying different shear angles can help to simulate different angles of the fruits<br>\n",
    "\n",
    "zoom_range: some of the fruits are zoomed in and zooming in/out of the fruits may help the model to better identify the fruits<br>\n",
    "\n",
    "horizontal_flip: flipping the images horizontally can help to introduce flipped orientation, helping the model to better identify it.<br>\n",
    "\n",
    "#### The list of parameter values below are tested for the respective image augmentation method\n",
    "rotation_range=[0, 30, 45, 60, 75]<br>\n",
    "width_shift_range=[0.0, 0.1, 0.2, 0.3, 0.4]<br>\n",
    "height_shift_range=[0.0, 0.1, 0.2, 0.3, 0.4]<br>\n",
    "shear_range=[0.0, 0.2, 0.4]<br>\n",
    "zoom_range=[0.0, 0.4, 0.6, 0.8, 1.0]<br>\n",
    "horizontal_flip=[False, True]<br>\n",
    "\n",
    "\n",
    "## The training methodology is shown below\n",
    "\n",
    "- The model's performance will be evalauted by measuring the accuracy on the test data\n",
    "- The graph of the training for each parameter will be shown below\n",
    "- If a parameter value results in an improvement in the accuracy of the test data, they will be selected along with other parameter values that have shown to be beneficial and tested together to determine if the image augmentation results in better model training\n",
    "- In the event the tested parameter or the combination of tested parameters do not results in an improvement in model training, a random combination of parameter values will be tested to check if they help to improve the accuracy which will help to reduce bias\n",
    "\n",
    "The baseline of the test accuracy on an untuned model with no data augmentation is shown below:\n",
    "\n",
    "The accuracy of the untuned model on the test dataset is: \n",
    "2/2 [==============================] - 1s 337ms/step - loss: 1.1511 - accuracy: 0.9000\n",
    "[1.1511120796203613, 0.8999999761581421]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3dffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(x_train, y_train, x_val, y_val, batch_size, aug_param):\n",
    "    \"\"\"\n",
    "    augments the training data by performing scaling and data augmentation to generate more data\n",
    "\n",
    "    Parameters:\n",
    "    x_train (np.array): the input training data for the cnn\n",
    "    y_train (np.array): the output training data for the cnn\n",
    "    x_val (np.array): the input validation data for the cnn\n",
    "    y_val (np.array): the input validation data for the cnn\n",
    "\n",
    "\n",
    "    Returns: \n",
    "    tuple: A tuple containing two `tf.keras.preprocessing.image.ImageDataGenerator` objects.\n",
    "    The first object generates batches of augmented training data, the second object generates batches of \n",
    "    validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=0.0, # default: 0.0\n",
    "        width_shift_range=0.0, # default: 0.0\n",
    "        height_shift_range=0.0, #default: 0.0\n",
    "        shear_range=0.0, #default: 0.0\n",
    "        zoom_range=0.0, #default: 0.0\n",
    "        horizontal_flip=aug_param #default: False\n",
    "        )\n",
    "    \n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Use the data generator to generate batches of augmented data\n",
    "    train_augment_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    val_augment_generator = ImageDataGenerator().flow(x_val, y_val, batch_size=batch_size)\n",
    "\n",
    "    return train_augment_generator, val_augment_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6093520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the values below for the loop to test\n",
    "\n",
    "# change this for the file name\n",
    "aug_param_name = \"horizontal_flip\"\n",
    "aug_params = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa88e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 13s 2s/step - loss: 185.9912 - accuracy: 0.2601 - val_loss: 48.7856 - val_accuracy: 0.2444\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 56.5358 - accuracy: 0.4753 - val_loss: 47.2649 - val_accuracy: 0.4667\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 21.3673 - accuracy: 0.5291 - val_loss: 9.3624 - val_accuracy: 0.5556\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 3.4944 - accuracy: 0.7623 - val_loss: 4.6601 - val_accuracy: 0.7556\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.1973 - accuracy: 0.8386 - val_loss: 3.2897 - val_accuracy: 0.8222\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.3704 - accuracy: 0.9238 - val_loss: 4.5413 - val_accuracy: 0.8444\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.1844 - accuracy: 0.9686 - val_loss: 6.3713 - val_accuracy: 0.8444\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0356 - accuracy: 0.9865 - val_loss: 3.8351 - val_accuracy: 0.8889\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0200 - accuracy: 0.9910 - val_loss: 3.4685 - val_accuracy: 0.8889\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.5352 - val_accuracy: 0.9111\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 4.1044e-04 - accuracy: 1.0000 - val_loss: 3.6416 - val_accuracy: 0.9111\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 4.7742e-04 - accuracy: 1.0000 - val_loss: 3.7042 - val_accuracy: 0.9111\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 5.3779e-04 - accuracy: 1.0000 - val_loss: 3.7359 - val_accuracy: 0.9111\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 5.2666e-04 - accuracy: 1.0000 - val_loss: 3.7463 - val_accuracy: 0.9111\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 3.0761e-04 - accuracy: 1.0000 - val_loss: 3.7492 - val_accuracy: 0.9111\n",
      "The accuracy of the untuned augmented data model on the test dataset is: \n",
      "2/2 [==============================] - 1s 242ms/step - loss: 3.0747 - accuracy: 0.8667\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 13s 2s/step - loss: 343.7492 - accuracy: 0.2332 - val_loss: 177.6347 - val_accuracy: 0.2889\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 91.8066 - accuracy: 0.2825 - val_loss: 37.7068 - val_accuracy: 0.3778\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 25.2488 - accuracy: 0.4484 - val_loss: 11.9900 - val_accuracy: 0.5556\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 5.7856 - accuracy: 0.5740 - val_loss: 4.1603 - val_accuracy: 0.5778\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.1225 - accuracy: 0.6652 - val_loss: 2.2800 - val_accuracy: 0.6222\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.1511 - accuracy: 0.6996 - val_loss: 1.4030 - val_accuracy: 0.7556\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.8677 - accuracy: 0.7489 - val_loss: 1.4025 - val_accuracy: 0.7556\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.3156 - accuracy: 0.8655 - val_loss: 1.7734 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.2675 - accuracy: 0.9148 - val_loss: 1.3602 - val_accuracy: 0.8222\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.2111 - accuracy: 0.9327 - val_loss: 1.4039 - val_accuracy: 0.8667\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.1232 - accuracy: 0.9641 - val_loss: 1.5148 - val_accuracy: 0.8889\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.1048 - accuracy: 0.9686 - val_loss: 1.5508 - val_accuracy: 0.8667\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0785 - accuracy: 0.9955 - val_loss: 1.4047 - val_accuracy: 0.8667\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0731 - accuracy: 0.9865 - val_loss: 1.4743 - val_accuracy: 0.8667\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0513 - accuracy: 0.9955 - val_loss: 1.5119 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 1.5439 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 1.5431 - val_accuracy: 0.8667\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 1.5993 - val_accuracy: 0.8667\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 1.5437 - val_accuracy: 0.8667\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 1.6126 - val_accuracy: 0.8667\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.6548 - val_accuracy: 0.8667\n",
      "The accuracy of the untuned augmented data model on the test dataset is: \n",
      "2/2 [==============================] - 1s 243ms/step - loss: 1.2319 - accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# run a for loop to test\n",
    "\n",
    "# initialise the variables used for hyperparameter tuning\n",
    "# number of epochs is set to 100, and earlystopping is used to reduce overfitting and stop \n",
    "# training automatically when there is not much improvements in the accuracy\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# generate steps per epoch\n",
    "num_of_samples = len(x_train)\n",
    "steps_per_epoch = num_of_samples // batch_size\n",
    "\n",
    "for params in aug_params:\n",
    "    \n",
    "    # augment the data\n",
    "    train_augment_generator, val_augment_generator = augment_data(x_train, y_train, x_val\n",
    "                                                                  , y_val, batch_size, params)\n",
    "    \n",
    "    # Initialise callbacks, earlystopping is used to reduce overfit \n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", restore_best_weights=True, patience=5)\n",
    "    # Create a csv logger callback and create a unique name for the logger file using datetime\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    data_aug_logger_name = f\"logger/data_aug_params/{aug_param_name}&{params}-{current_time}.log\"\n",
    "    data_aug_logger = tf.keras.callbacks.CSVLogger(data_aug_logger_name, separator=\",\", append=False)\n",
    "\n",
    "    # Create a list of the loggers to fit in the model\n",
    "    aug_data_callbacks = [earlyStopping, data_aug_logger]\n",
    "    \n",
    "    # create the model\n",
    "    aug_data_model = created_untuned_model(x_input_shape, output_layer)\n",
    "    aug_data_history = train_aug_gen_model(aug_data_model, train_augment_generator, batch_size, epochs\n",
    "                                           , aug_data_callbacks, val_augment_generator, steps_per_epoch)\n",
    "    \n",
    "    # test the model\n",
    "    print(\"The accuracy of the untuned augmented data model on the test dataset is: \")\n",
    "    test_results = test_model(aug_data_model, x_test, y_test)\n",
    "    \n",
    "    # save the test results to a csv file\n",
    "    csv_fname = f\"logger/data_aug_params_test_acc/aug_params_test.csv\"\n",
    "\n",
    "    # Assuming you have the following values\n",
    "    param_name = aug_param_name\n",
    "    value = params\n",
    "    loss = test_results[0]\n",
    "    accuracy = test_results[1]\n",
    "\n",
    "    # Open the CSV file in append mode\n",
    "    with open(csv_fname, \"a\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writerow([\"param\", \"value\", \"loss\", \"accuracy\"])\n",
    "\n",
    "        # Write a new entry to the CSV file\n",
    "        writer.writerow([param_name, value, loss, accuracy])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
